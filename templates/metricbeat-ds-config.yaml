{{- $elasticsearchServiceName := printf "%s-%s" .Release.Name "elasticsearch" | trunc 63 -}}
{{- $logstashServiceName := printf "%s-%s" .Release.Name "elasticsearch-logstash" | trunc 63 -}}
{{- $redisServiceName := printf "%s-%s" .Release.Name "elasticsearch-redis" | trunc 63 -}}
{{- $kibanaServiceName := printf "%s-%s" .Release.Name "elasticsearch-kibana" | trunc 63 -}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "fullname" . }}-metricbeat-daemonset-config
  labels:
    app: {{ template "fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
    kubernetes.io/cluster-service: "true"
data:
  metricbeat.yml: |-
    name: ${NODE_NAME}
    fields:
      nodePool: ${POOL_NAME}

    max_start_delay: {{ .Values.beats.max_start_delay }}

    queue.mem:
      events: {{ .Values.beats.queue.events }}
      flush:
        min_events: {{ .Values.beats.queue.min_events }}
        timeout: {{ .Values.beats.queue.timeout }}

    metricbeat.modules:
      - module: system
        period: 10s
        metricsets:
          - cpu
          - load
          - memory
          - network
          - process
          - process_summary
          #- core
          #- diskio
          #- socket
        processes: ['.*']
        process.include_top_n:
          by_cpu: 5      # include top 5 processes by CPU
          by_memory: 5   # include top 5 processes by memory

      - module: system
        enabled: false
        period: 1m
        metricsets:
          - filesystem
          - fsstat
        processors:
        - drop_event.when.regexp:
            system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)'

      - module: kubernetes
        metricsets:
          - node
          - system
          - pod
          - container
          - volume
        period: 10s
        hosts: ["${NODE_NAME}:10255"]

    metricbeat.autodiscover:
      providers:
        - type: kubernetes
          templates:
            - condition:
                contains:
                  kubernetes.container.image: docker.elastic.co/elasticsearch/elasticsearch-oss
              config:
                - module: elasticsearch
                  metricsets:
                    - node
                    - node_stats
                  period: 10s
                  hosts: "${data.host}:9200"
            - condition:
                contains:
                  kubernetes.container.image: us.gcr.io/evident-plane-94217/elasticsearch
              config:
                - module: elasticsearch
                  metricsets:
                    - node
                    - node_stats
                  period: 10s
                  hosts: "${data.host}:9200"
            - condition:
                contains:
                  kubernetes.container.image: quay.io/pires/docker-elasticsearch-kubernetes
              config:
                - module: elasticsearch
                  metricsets: 
                    - node
                    - node_stats
                  period: 10s
                  hosts: "${data.host}:9200"

            - condition:
                contains:
                  kubernetes.container.image: mongo
              config:
                - module: mongodb
                  metricsets: 
                    - collstats
                    - dbstats
                    - status
                  period: 10s
                  hosts: "${data.host}:27017"

            - condition:
                contains:
                  kubernetes.container.image: mysql
              config:
                - module: mysql
                  metricsets: 
                    - status
                  period: 10s
                  hosts: "tcp(${data.host}:3306)/"
                  username: root
                  password: 6kCGBg4TdUQH3ng2

            # Unfortunately, http://nginx:80/nginx_status that uses stub_status is only available to 127.0.0.1
            # - condition:
            #     contains:
            #       kubernetes.container.image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
            #   config:
            #     - module: nginx
            #       metricsets: 
            #         - stubstatus
            #       period: 10s
            #       hosts: "http://${data.host}:80"
            #       server_status_path: "nginx_status"
                  
            - condition:
                contains:
                  kubernetes.container.image: redis
              config:
                - module: redis
                  metricsets: 
                    - info
                    - keyspace
                  period: 10s
                  hosts: "${data.host}:6379"

            - condition:
                contains:
                  kubernetes.container.image: rabbitmq
              config:
                - module: rabbitmq
                  metricsets: 
                    - node
                    - queue
                  period: 10s
                  hosts: "${data.host}:6379"
                  username: guest
                  password: guest

    logging.metrics.enabled: false

    processors:
      - add_cloud_metadata: ~

    {{- if .Values.logstash.enabled }}
    output.logstash:
      hosts: ['{{ $logstashServiceName }}:{{ .Values.logstash.beatsPort }}']
      bulk_max_size: {{ .Values.beats.output.max_bulk_size }}
      slow_start: true
    {{- else if .Values.redis.enabled }}
    output.redis:
      hosts: ['{{ $redisServiceName }}:{{ .Values.redis.port }}']
      bulk_max_size: {{ .Values.beats.output.max_bulk_size }}
    {{- else }}
    output.elasticsearch:
      hosts: ['http://{{ $elasticsearchServiceName }}:{{ .Values.service.httpPort }}']
      bulk_max_size: {{ .Values.beats.output.max_bulk_size }}
    {{- end }}
    
    {{- if .Values.kibana.enabled }}      
    setup.kibana.host: "http://{{ $kibanaServiceName }}:{{ .Values.kibana.httpPort }}"
    setup.dashboards.enabled: {{ .Values.metricbeat.setupDashboards }}
    {{- end }}
